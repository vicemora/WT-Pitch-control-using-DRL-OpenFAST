clear all;
clc;
%% Path to files
FAST_InputFileName = '..\..\WT Model\IEA3.4-RWT-OpenFAST\IEA-3.4-130-RWT.fst';

%% Simulink configuration
mdl = 'OpenLoop';
agentblk = [mdl '/RL Agent'];
open_system(mdl); 
% rng('default')

%% Create RL enviroment
n_states = 5; % [power_error, power_error', wind_speed, current_pitch, rotor_speed]
observationInfo = rlNumericSpec([n_states 1],'LowerLimit', ...
            -inf*ones(n_states,1),'UpperLimit',inf*ones(n_states,1));
n_actions = 301;
actions = linspace(0,pi/6,n_actions); %[0,0.1°,0.2°,...,30°]
actionInfo = rlFiniteSetSpec(actions);
env = rlSimulinkEnv(mdl,agentblk,observationInfo,actionInfo);

%% Create RL-DDQN agent
nI = observationInfo.Dimension(1);  %Number of inputs                      
nO = numel(actionInfo.Elements); %Number of outputs
hidden_1 = 64;
hidden_2 = 128;
dnn = [
    featureInputLayer(nI,'Normalization','none','Name','state')
    fullyConnectedLayer(hidden_1,'Name','fc1')
    reluLayer('Name','relu1')
    fullyConnectedLayer(hidden_2,'Name','fc2')
    reluLayer('Name','relu2')
    fullyConnectedLayer(nO,'Name','fc3')];
dnn = dlnetwork(dnn);
summary(dnn)

criticOptions = rlRepresentationOptions('LearnRate',0.01,'GradientThreshold',1,'L2RegularizationFactor',1e-4);
critic = rlQValueRepresentation(dnn,observationInfo,actionInfo,'Observation',{'state'},criticOptions);
agentOpts = rlDQNAgentOptions(...
    'SampleTime',Ts,... 
    'UseDoubleDQN',true,...
    'TargetSmoothFactor',1e-2,...
    'TargetUpdateFrequency',50,...
    'DiscountFactor',0.99,...
    'ExperienceBufferLength',1e6,...
    'MiniBatchSize',128);

agent = rlDQNAgent(critic,agentOpts);
agent.ExperienceBuffer=rlPrioritizedReplayMemory(observationInfo,actionInfo);
agent.AgentOptions.InfoToSave.ExperienceBuffer=true;
agent.AgentOptions.InfoToSave.Optimizer=true;
agent.AgentOptions.InfoToSave.PolicyState=true;
agent.AgentOptions.InfoToSave.Target=true;


%% RL training configuration

maxepisodes = 1; %non-sp task 
maxsteps = ceil((TMax/maxepisodes)/Ts);
training_Opts = rlTrainingOptions(...
'MaxEpisodes',maxepisodes, ...
'MaxStepsPerEpisode',maxsteps, ...
'Verbose',true,...
'StopTrainingCriteria','EpisodeCount',...
'StopTrainingValue',maxepisodes,...
'SaveAgentCriteria','EpisodeCount',...
'SaveAgentValue',maxepisodes,...
'SaveAgentDirectory', 'policy_refinement'); %directory to save the agent



Ts=0.025;
TMax=6000; 




%% create agent   


%agentOpts.EpsilonGreedyExploration.EpsilonDecay = power(10,-3.25);
agent = rlDQNAgent(critic,agentOpts);
agent.ExperienceBuffer=rlPrioritizedReplayMemory(observationInfo,actionInfo);
agent.AgentOptions.InfoToSave.ExperienceBuffer=true;
agent.AgentOptions.InfoToSave.Optimizer=true;
agent.AgentOptions.InfoToSave.PolicyState=true;
agent.AgentOptions.InfoToSave.Target=true;
agent.UseExplorationPolicy=true;

%% training options
agent=load('savedAgents\agentes_exp4_0\Agent1_0.mat','saved_agent');
agent=agent.saved_agent;
agent.SampleTime=Ts;
%% set a new agent with the old agent params

old_agent = agent.saved_agent;

% Obtener su critic y pesos
old_critic = getCritic(old_agent);
old_params = getLearnableParameters(old_critic);

% Definir red igual a la usada antes
dnn = [
    featureInputLayer(n_states,'Normalization','none','Name','state')
    fullyConnectedLayer(64,'Name','fc1')
    reluLayer('Name','relu1')
    fullyConnectedLayer(128,'Name','fc2')
    reluLayer('Name','relu2')
    fullyConnectedLayer(n_act,'Name','fc3')];
dnn = dlnetwork(dnn);

% Crear nuevo critic con opciones iguales al anterior
criticOpts = rlRepresentationOptions('LearnRate',0.01,'GradientThreshold',1,'L2RegularizationFactor',1e-4, UseDevice = "gpu");
%critic = rlQValueRepresentation(dnn, observationInfo, actionInfo,'Observation',{'state'}, 'Options',criticOpts);

critic = setLearnableParameters(old_critic, old_params);

agentOpts = rlDQNAgentOptions(...
    'SampleTime',Ts,...
    'UseDoubleDQN',true,...
    'TargetUpdateFrequency',50,...
    'DiscountFactor',0.99,...
    'MiniBatchSize',128,...
    'ExperienceBufferLength',1e6);

agent = rlDQNAgent(critic, agentOpts);


%%
agent.UseExplorationPolicy = 1;
agent.AgentOptions.EpsilonGreedyExploration.Epsilon = 0.1;
agent.AgentOptions.ResetExperienceBufferBeforeTraining = 1;
agent.AgentOptions.NumEpoch = 1000;
agent.AgentOptions.EpsilonGreedyExploration.EpsilonDecay=power(10,-3.25);

%% rate and episodes
rate=1;
maxepisodes = 1;
maxsteps = ceil((TMax/maxepisodes)/Ts);
%%
trainOpts = rlTrainingOptions(...
'MaxEpisodes',maxepisodes, ...
'MaxStepsPerEpisode',maxsteps, ...
'Verbose',true,...
'Plots','training-progress',...
'StopTrainingCriteria','EpisodeCount',...
'StopTrainingValue',maxepisodes,...
'SaveAgentCriteria','EpisodeCount',...
'SaveAgentValue',maxepisodes,...
'SaveAgentDirectory', 'policy_refinement' );

%% params before
criticBefore = getCritic(agent);
paramsBefore = getLearnableParameters(criticBefore);
%% train
trainingStats = train(agent,env,trainOpts); 
%% params after
%agent=load("policy_refinement\Agent1.mat",'saved_agent');
agent=load('savedAgents\agentes_exp4_0\Agent1_4.mat','saved_agent');
agent=agent.saved_agent;
criticAfter = getCritic(agent);
paramsAfter = getLearnableParameters(criticAfter);

%%
for i = 1:numel(paramsBefore)
    diff = abs(paramsAfter{i} - paramsBefore{i});
    meanDiff = mean(diff(:)); 
    fprintf("Capa %d - Cambio medio en pesos: %.6f\n", i, meanDiff);
end
%% simulate DQN agent
TMax=1000; 
maxsteps = ceil(TMax/Ts);
rate=0.4;
agent=load("savedAgents\agentes_exp4_0\Agent1_4.mat",'saved_agent');
%agent=load("policy_refinement\Agent1.mat",'saved_agent');

%agent=load("savedAgents\prueba_agente.mat",'agent');
%agent=load("savedAgents\Agent_recompensaconf1.mat",'saved_agent');
 
%agent=agent.agent;
agent=agent.saved_agent;
%%
kp = -0.001;
agent.SampleTime=Ts; 
simOptions = rlSimulationOptions('MaxSteps',maxsteps);
experience = sim(env,agent,simOptions);

%% graficar reward


tiempo = reward(:,1);
reward = reward(:,2);  

% Calcular la suma acumulada del reward
reward_acumulado = cumsum(reward);

% Graficar
figure;
plot(tiempo, reward_acumulado, '-o', 'LineWidth', 2);
xlabel('Tiempo (s)');
ylabel('Cumulative Reward Acumulado');
title('Cumulative Reward Acumulado en el Tiempo');
grid on;